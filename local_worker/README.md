---

title: MeetMate AI API
sdk: docker
app_port: 7860
---

Backend AI microservice for MeetMate.

# üéôÔ∏è MeetMate Model Service

**Microservice API** cho voice diarization, transcription, v√† speaker embedding - deploy tr√™n **Hugging Face Spaces**.

---

## üì¶ T·ªïng quan

Service n√†y cung c·∫•p 3 model ch√≠nh:

1. **Speaker Diarization** (pyannote.audio) - Ph√¢n bi·ªát ng∆∞·ªùi n√≥i
2. **Speech Transcription** (Whisper) - Chuy·ªÉn gi·ªçng n√≥i th√†nh text
3. **Speaker Embedding** (pyannote.audio) - Tr√≠ch xu·∫•t voice fingerprint

---

## üöÄ Quickstart - Local Development

### 1. Install Dependencies

```bash
pip install -r requirements_hf.txt
```

### 2. Setup Environment

```bash
# Copy example env
cp .env.example .env

# Edit .env and add your HF token
# Get token from: https://huggingface.co/settings/tokens
nano .env
```

**Accept model licenses:**
- [pyannote/speaker-diarization-3.1](https://huggingface.co/pyannote/speaker-diarization-3.1)
- [pyannote/segmentation-3.0](https://huggingface.co/pyannote/segmentation-3.0)

### 3. Run Server

```bash
python app.py
```

Visit: http://localhost:7860 (Swagger UI)

---

## üß™ Testing

```bash
# Test with your audio file
python test_api.py

# Or use curl
curl -X POST "http://localhost:7860/api/diarize" \
  -F "audio_file=@test_audio.wav"
```

---

## ‚òÅÔ∏è Deploy to Hugging Face Spaces

### Option 1: Via Web UI

1. Create Space: https://huggingface.co/new-space
2. Choose **Docker SDK**
3. Upload all files from `local_worker/`
4. Add secret: `HF_TOKEN`
5. Space auto-builds! üéâ

### Option 2: Via CLI

```bash
# Install huggingface_hub
pip install huggingface_hub

# Login
huggingface-cli login

# Create space
huggingface-cli repo create meetmate-models --type space --space_sdk docker

# Upload files
cd local_worker
git init
git remote add space https://huggingface.co/spaces/YOUR_USERNAME/meetmate-models
git add .
git commit -m "Initial commit"
git push --force space main
```

---

## üìñ API Documentation

### 1. Health Check

```bash
GET /health
```

**Response:**
```json
{
  "status": "healthy",
  "models": {
    "diarization": true,
    "transcription": true,
    "speaker_embedding": false
  },
  "gpu_available": false
}
```

---

### 2. Speaker Diarization

```bash
POST /api/diarize
Content-Type: multipart/form-data

audio_file: <file>
```

**Response:**
```json
{
  "segments": [
    {
      "speaker": "SPEAKER_00",
      "start": 0.5,
      "end": 3.2,
      "confidence": 1.0
    }
  ],
  "num_speakers": 2,
  "duration": 10.5
}
```

---

### 3. Transcription

```bash
POST /api/transcribe
Content-Type: multipart/form-data

audio_file: <file>
language: vi  # vi, en, or auto
with_diarization: false  # true to include speakers
```

**Response:**
```json
{
  "text": "Xin ch√†o c√°c b·∫°n...",
  "segments": [
    {
      "text": "Xin ch√†o c√°c b·∫°n",
      "start": 0.0,
      "end": 2.5,
      "speaker": null
    }
  ],
  "language": "vi",
  "duration": 10.5
}
```

---

### 4. Speaker Embedding

```bash
POST /api/speaker-embedding
Content-Type: multipart/form-data

audio_file: <file>
```

**Response:**
```json
{
  "embedding": [0.123, -0.456, ...],
  "dimension": 512,
  "duration": 5.2
}
```

---

## üîå Integration v·ªõi MeetMate Backend

### Backend Configuration

Add to `backend/.env`:

```env
MEETMATE_MODELS_API=https://YOUR_USERNAME-meetmate-models.hf.space
```

### Example Usage

```python
# backend/app/services/model_service.py
import httpx
from app.core.config import get_settings

settings = get_settings()


async def diarize_post_meeting_audio(audio_file_path: str) -> list[dict]:
    """
    Diarize audio file sau cu·ªôc h·ªçp
    
    Used in Post-Meeting ƒë·ªÉ refine transcripts
    """
    async with httpx.AsyncClient(timeout=120.0) as client:
        with open(audio_file_path, "rb") as f:
            response = await client.post(
                f"{settings.meetmate_models_api}/api/diarize",
                files={"audio_file": ("audio.wav", f, "audio/wav")}
            )
        
        response.raise_for_status()
        result = response.json()
        
        return result["segments"]


async def transcribe_with_whisper(audio_file_path: str) -> dict:
    """
    Transcribe audio v·ªõi Whisper (reference cho Post-Meeting)
    
    So s√°nh v·ªõi SmartVoice STT ƒë·ªÉ improve accuracy
    """
    async with httpx.AsyncClient(timeout=180.0) as client:
        with open(audio_file_path, "rb") as f:
            response = await client.post(
                f"{settings.meetmate_models_api}/api/transcribe",
                files={"audio_file": ("audio.wav", f, "audio/wav")},
                data={
                    "language": "vi",
                    "with_diarization": True
                }
            )
        
        response.raise_for_status()
        return response.json()
```

---

## üìä Performance

| Task | Model | CPU (1 min audio) | GPU T4 (1 min audio) |
|------|-------|-------------------|----------------------|
| Diarization | pyannote 3.1 | ~120s | ~12s |
| Transcription | Whisper base | ~30s | ~6s |
| Embedding | pyannote emb | <1s | <0.5s |

**Recommendations:**
- **Free Tier (CPU):** Testing only
- **GPU T4 ($0.60/hr):** Production-ready
- **Process shorter chunks:** Split long meetings into 5-10 min segments

---

## üéØ Use Cases in MeetMate

### 1. **Post-Meeting Refinement**
- Diarize full meeting audio
- Cross-check v·ªõi realtime STT
- Merge speakers th√†nh consistent labels

### 2. **Whisper Reference**
- Transcribe sau h·ªçp v·ªõi Whisper
- So s√°nh accuracy v·ªõi SmartVoice
- Use as ground truth for quality metrics

### 3. **Speaker Verification**
- Extract embeddings t·ª´ audio samples
- Match speakers across meetings
- Build speaker profiles

---

## üîß Troubleshooting

### Models kh√¥ng load ƒë∆∞·ª£c

```
‚ùå Error: HF_TOKEN is required
```

**Fix:** Set `HF_TOKEN` environment variable

---

### Out of memory

```
‚ùå CUDA out of memory
```

**Fix:**
- Process shorter audio segments
- Use smaller Whisper model (`tiny`, `base`)
- Upgrade Space to larger GPU

---

### Slow inference on CPU

**Fix:**
- Upgrade to GPU Space
- Enable `accelerate` library
- Use quantized models

---

## üìÇ Project Structure

```
local_worker/
‚îú‚îÄ‚îÄ app.py                   # Main FastAPI app
‚îú‚îÄ‚îÄ Dockerfile               # HF Space Docker config
‚îú‚îÄ‚îÄ requirements_hf.txt      # Dependencies
‚îú‚îÄ‚îÄ README.md               # This file
‚îú‚îÄ‚îÄ README_HF.md            # HF Space README
‚îú‚îÄ‚îÄ test_api.py             # Test script
‚îú‚îÄ‚îÄ .env.example            # Environment template
‚îÇ
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ diarization_model.py      # Pyannote diarization
‚îÇ   ‚îú‚îÄ‚îÄ transcription_model.py    # Whisper
‚îÇ   ‚îî‚îÄ‚îÄ speaker_embedding_model.py # Speaker embeddings
‚îÇ
‚îî‚îÄ‚îÄ utils/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îî‚îÄ‚îÄ audio_utils.py        # Audio processing

# Legacy files (no longer used for HF Space):
‚îú‚îÄ‚îÄ worker.py              # Old streaming worker
‚îú‚îÄ‚îÄ audio_buffer.py        # Old buffer
‚îú‚îÄ‚îÄ api_client.py          # Old client
‚îî‚îÄ‚îÄ speaker_registry.py    # Old registry
```

---

## üÜò Support

- **HF Spaces Docs:** https://huggingface.co/docs/hub/spaces
- **pyannote.audio:** https://github.com/pyannote/pyannote-audio
- **Whisper:** https://github.com/openai/whisper

---

## üìú License

Apache 2.0

---

**Built for VNPT AI Hackathon 2025 | MeetMate Project** üöÄ

=======
title: Meetmate
emoji: üëÄ
colorFrom: purple
colorTo: indigo
sdk: docker
pinned: false
---

Check out the configuration reference at https://huggingface.co/docs/hub/spaces-config-reference
>>>>>>> 4e3325fdf8225a31d45dec28174441e4ec6c0842
