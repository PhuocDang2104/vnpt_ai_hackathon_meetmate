"""
Google Gemini AI Client
Handles all AI interactions using Google's Gemini API
"""
import google.generativeai as genai
from typing import Optional, List, Dict, Any
from app.core.config import get_settings

settings = get_settings()


def get_gemini_client():
    """Initialize and return Gemini client"""
    if not settings.gemini_api_key:
        return None
    
    genai.configure(api_key=settings.gemini_api_key)
    return genai.GenerativeModel(settings.gemini_model)


def is_gemini_available() -> bool:
    """Check if Gemini API is configured and working"""
    if not settings.gemini_api_key or settings.gemini_api_key == '':
        print("[Gemini] No API key configured")
        return False
    try:
        genai.configure(api_key=settings.gemini_api_key)
        # Try a simple test call
        model = genai.GenerativeModel(settings.gemini_model)
        response = model.generate_content("test", generation_config=genai.types.GenerationConfig(max_output_tokens=10))
        print(f"[Gemini] API working with model: {settings.gemini_model}")
        return True
    except Exception as e:
        print(f"[Gemini] API error: {e}")
        return False


class GeminiChat:
    """Gemini Chat Session Manager"""
    
    def __init__(self, system_prompt: Optional[str] = None):
        self.model = get_gemini_client()
        self.system_prompt = system_prompt or self._default_system_prompt()
        self.history: List[Dict[str, str]] = []
    
    def _default_system_prompt(self) -> str:
        return """Báº¡n lÃ  MeetMate AI Assistant - trá»£ lÃ½ thÃ´ng minh cho PMO (Project Management Office) cá»§a ngÃ¢n hÃ ng LPBank.

Nhiá»‡m vá»¥ cá»§a báº¡n:
1. Há»— trá»£ chuáº©n bá»‹ cuá»™c há»p (Pre-meeting): Gá»£i Ã½ agenda, tÃ i liá»‡u, ngÆ°á»i tham gia
2. Há»— trá»£ trong cuá»™c há»p (In-meeting): Ghi chÃ©p, phÃ¡t hiá»‡n action items, decisions, risks
3. Há»— trá»£ sau cuá»™c há»p (Post-meeting): Táº¡o biÃªn báº£n, theo dÃµi tasks, Q&A

NguyÃªn táº¯c tráº£ lá»i:
- Tráº£ lá»i báº±ng tiáº¿ng Viá»‡t, ngáº¯n gá»n, chuyÃªn nghiá»‡p
- Sá»­ dá»¥ng markdown Ä‘á»ƒ format (bold, bullet points)
- Náº¿u Ä‘Æ°á»£c há»i vá» policy/quy Ä‘á»‹nh, trÃ­ch dáº«n nguá»“n cá»¥ thá»ƒ
- Náº¿u khÃ´ng cháº¯c cháº¯n, nÃ³i rÃµ "TÃ´i khÃ´ng cÃ³ thÃ´ng tin vá» Ä‘iá»u nÃ y"
- Vá»›i cÃ¢u há»i vá» dá»± Ã¡n, tham chiáº¿u cÃ¡c tÃ i liá»‡u ná»™i bá»™

Báº¡n cÃ³ kiáº¿n thá»©c vá»:
- CÃ¡c dá»± Ã¡n CNTT cá»§a LPBank (Core Banking, Mobile Banking, LOS, KYC)
- Quy trÃ¬nh PMO, SCRUM, Agile
- Compliance vÃ  regulations (NHNN Circular 09/2020, security policies)
- Microsoft Teams, Jira, Planner integration"""

    async def chat(self, message: str, context: Optional[str] = None) -> str:
        """Send a message and get response"""
        if not self.model:
            return self._mock_response(message)
        
        try:
            # Build prompt with context
            full_prompt = self.system_prompt
            
            if context:
                full_prompt += f"\n\nContext cuá»™c há»p:\n{context}"
            
            # Add history
            if self.history:
                full_prompt += "\n\nLá»‹ch sá»­ há»™i thoáº¡i:"
                for h in self.history[-5:]:  # Last 5 messages
                    full_prompt += f"\nUser: {h['user']}\nAssistant: {h['assistant']}"
            
            full_prompt += f"\n\nUser: {message}\nAssistant:"
            
            # Generate response
            response = self.model.generate_content(
                full_prompt,
                generation_config=genai.types.GenerationConfig(
                    temperature=settings.ai_temperature,
                    max_output_tokens=settings.ai_max_tokens,
                )
            )
            
            assistant_message = response.text
            
            # Save to history
            self.history.append({
                'user': message,
                'assistant': assistant_message
            })
            
            return assistant_message
            
        except Exception as e:
            import traceback
            print(f"[Gemini] Chat error: {e}")
            print(f"[Gemini] Traceback: {traceback.format_exc()}")
            return self._mock_response(message)
    
    def _mock_response(self, message: str) -> str:
        """Fallback mock response when API is not available"""
        message_lower = message.lower()
        
        if 'retention' in message_lower or 'lÆ°u trá»¯' in message_lower:
            return """Theo **ThÃ´ng tÆ° 09/2020/TT-NHNN** vá» quáº£n lÃ½ rá»§i ro CNTT:

- **Dá»¯ liá»‡u giao dá»‹ch (transaction logs)**: LÆ°u trá»¯ tá»‘i thiá»ƒu **10 nÄƒm**
- **Dá»¯ liá»‡u khÃ¡ch hÃ ng**: LÆ°u trá»¯ **5 nÄƒm** sau khi káº¿t thÃºc quan há»‡
- **Logs há»‡ thá»‘ng**: Tá»‘i thiá»ƒu **3 nÄƒm**

ğŸ“Œ *Nguá»“n: Äiá»u 15, ThÃ´ng tÆ° 09/2020/TT-NHNN*"""

        elif 'security' in message_lower or 'báº£o máº­t' in message_lower:
            return """Theo **Security Policy v3.0** cá»§a LPBank:

**Encryption Requirements:**
- Data at rest: **AES-256**
- Data in transit: **TLS 1.3**

**Access Control:**
- Multi-factor authentication (MFA) báº¯t buá»™c
- Role-based access control (RBAC)
- Session timeout: 15 phÃºt

**Audit:**
- Penetration testing: Quarterly
- Security review: Monthly

ğŸ“Œ *Nguá»“n: LPBank Security Policy v3.0, Section 4*"""

        elif 'agenda' in message_lower or 'chÆ°Æ¡ng trÃ¬nh' in message_lower:
            return """Dá»±a trÃªn loáº¡i cuá»™c há»p, tÃ´i Ä‘á» xuáº¥t **chÆ°Æ¡ng trÃ¬nh** sau:

**1. Khai máº¡c & Äiá»ƒm danh** (5 phÃºt)
- Chá»§ tá»‹ch khai máº¡c
- XÃ¡c nháº­n quorum

**2. BÃ¡o cÃ¡o tiáº¿n Ä‘á»™** (15 phÃºt)
- PM trÃ¬nh bÃ y status
- Demo features má»›i

**3. Tháº£o luáº­n Issues** (20 phÃºt)
- Blockers
- Risks

**4. Quyáº¿t Ä‘á»‹nh & Action Items** (10 phÃºt)
- Vote cÃ¡c decisions
- Assign owners & deadlines

**5. Káº¿t luáº­n** (5 phÃºt)

â±ï¸ *Tá»•ng thá»i gian: ~55 phÃºt*"""

        elif 'risk' in message_lower or 'rá»§i ro' in message_lower:
            return """**CÃ¡c rá»§i ro chÃ­nh cáº§n lÆ°u Ã½:**

ğŸ”´ **Critical:**
- Delay go-live Core Banking áº£nh hÆ°á»Ÿng chiáº¿n dá»‹ch cuá»‘i nÄƒm

ğŸŸ  **High:**
- 3 security issues tá»« Pentest chÆ°a fix
- Resource shortage trong Q4

ğŸŸ¡ **Medium:**
- Integration vá»›i LOS cÃ³ thá»ƒ gáº·p váº¥n Ä‘á» performance
- Documentation chÆ°a hoÃ n thiá»‡n

**Äá» xuáº¥t:**
1. Escalate resource issue lÃªn Steering
2. Set deadline cá»©ng cho security fixes
3. ThÃªm performance testing cho integration

ğŸ“Œ *Tham kháº£o: Risk Register Dashboard*"""

        else:
            return """Cáº£m Æ¡n cÃ¢u há»i cá»§a báº¡n. Dá»±a trÃªn knowledge base cá»§a MeetMate, tÃ´i cÃ³ thá»ƒ há»— trá»£ báº¡n vá»:

ğŸ“‹ **Pre-meeting:**
- Táº¡o agenda tá»± Ä‘á»™ng
- Gá»£i Ã½ tÃ i liá»‡u pre-read
- Äá» xuáº¥t ngÆ°á»i tham gia

ğŸ™ï¸ **In-meeting:**
- Ghi chÃ©p real-time
- PhÃ¡t hiá»‡n Action Items, Decisions, Risks
- Há»i Ä‘Ã¡p policy/documents

ğŸ“ **Post-meeting:**
- Táº¡o biÃªn báº£n tá»± Ä‘á»™ng
- Sync tasks vá»›i Jira/Planner
- Q&A vá» ná»™i dung cuá»™c há»p

Báº¡n muá»‘n tÃ´i há»— trá»£ Ä‘iá»u gÃ¬ cá»¥ thá»ƒ?"""


class MeetingAIAssistant:
    """AI Assistant specifically for meeting context"""
    
    def __init__(self, meeting_id: str, meeting_context: Optional[Dict[str, Any]] = None):
        self.meeting_id = meeting_id
        self.meeting_context = meeting_context or {}
        self.chat = GeminiChat()
    
    def _build_context(self) -> str:
        """Build context string from meeting data"""
        ctx_parts = []
        
        if self.meeting_context.get('title'):
            ctx_parts.append(f"Cuá»™c há»p: {self.meeting_context['title']}")
        
        if self.meeting_context.get('type'):
            ctx_parts.append(f"Loáº¡i: {self.meeting_context['type']}")
        
        if self.meeting_context.get('project'):
            ctx_parts.append(f"Dá»± Ã¡n: {self.meeting_context['project']}")
        
        if self.meeting_context.get('agenda'):
            ctx_parts.append(f"Agenda: {self.meeting_context['agenda']}")
        
        if self.meeting_context.get('transcript'):
            ctx_parts.append(f"Transcript (trÃ­ch): {self.meeting_context['transcript'][:500]}...")
        
        return "\n".join(ctx_parts)
    
    async def ask(self, question: str) -> str:
        """Ask a question with meeting context"""
        context = self._build_context()
        return await self.chat.chat(question, context)
    
    async def generate_agenda(self, meeting_type: str) -> str:
        """Generate agenda based on meeting type"""
        prompt = f"""Táº¡o chÆ°Æ¡ng trÃ¬nh cuá»™c há»p chi tiáº¿t cho loáº¡i: {meeting_type}

YÃªu cáº§u:
- Má»—i má»¥c cÃ³: sá»‘ thá»© tá»±, tiÃªu Ä‘á», thá»i lÆ°á»£ng (phÃºt), ngÆ°á»i trÃ¬nh bÃ y
- Tá»•ng thá»i gian khoáº£ng 60 phÃºt
- Format: JSON array vá»›i fields: order, title, duration_minutes, presenter"""
        
        return await self.chat.chat(prompt)
    
    async def extract_action_items(self, transcript: str) -> str:
        """Extract action items from transcript"""
        prompt = f"""PhÃ¢n tÃ­ch transcript sau vÃ  trÃ­ch xuáº¥t cÃ¡c Action Items:

{transcript[:2000]}

Format output JSON:
[
  {{
    "description": "MÃ´ táº£ task",
    "owner": "TÃªn ngÆ°á»i Ä‘Æ°á»£c giao (náº¿u cÃ³)",
    "deadline": "Deadline (náº¿u Ä‘Æ°á»£c Ä‘á» cáº­p)",
    "priority": "high/medium/low"
  }}
]"""
        
        return await self.chat.chat(prompt)
    
    async def extract_decisions(self, transcript: str) -> str:
        """Extract decisions from transcript"""
        prompt = f"""PhÃ¢n tÃ­ch transcript sau vÃ  trÃ­ch xuáº¥t cÃ¡c Quyáº¿t Ä‘á»‹nh (Decisions):

{transcript[:2000]}

Format output JSON:
[
  {{
    "description": "Ná»™i dung quyáº¿t Ä‘á»‹nh",
    "rationale": "LÃ½ do (náº¿u cÃ³)",
    "confirmed_by": "NgÆ°á»i xÃ¡c nháº­n"
  }}
]"""
        
        return await self.chat.chat(prompt)
    
    async def extract_risks(self, transcript: str) -> str:
        """Extract risks from transcript"""
        prompt = f"""PhÃ¢n tÃ­ch transcript sau vÃ  trÃ­ch xuáº¥t cÃ¡c Rá»§i ro (Risks):

{transcript[:2000]}

Format output JSON:
[
  {{
    "description": "MÃ´ táº£ rá»§i ro",
    "severity": "critical/high/medium/low",
    "mitigation": "Biá»‡n phÃ¡p giáº£m thiá»ƒu (náº¿u cÃ³)"
  }}
]"""
        
        return await self.chat.chat(prompt)
    
    async def generate_summary(self, transcript: str) -> str:
        """Generate meeting summary"""
        prompt = f"""Táº¡o tÃ³m táº¯t cuá»™c há»p tá»« transcript sau:

{transcript[:3000]}

Format:
## TÃ³m táº¯t cuá»™c há»p

### CÃ¡c Ä‘iá»ƒm chÃ­nh
- ...

### Quyáº¿t Ä‘á»‹nh
- ...

### Action Items
- ...

### Rá»§i ro Ä‘Æ°á»£c Ä‘á» cáº­p
- ...

### BÆ°á»›c tiáº¿p theo
- ..."""
        
        return await self.chat.chat(prompt)

